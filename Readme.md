# ğŸŒ Scalable Website Crawling & Analytics Pipeline

An **end-to-end, scalable data engineering pipeline** that crawls multiple websites, extracts structured content, aggregates analytics, and visualizes insights through a professional Streamlit dashboard â€” all orchestrated using **Apache Airflow** and **Docker**.

---

##  Project Overview

This project demonstrates how to design and implement a **production-style data pipeline** with clear separation of concerns:

- **Config-driven ingestion** (scale from 1 â†’ N websites)
- **Modular crawling & extraction logic**
- **Airflow-based orchestration**
- **Aggregation layer for analytics**
- **Decoupled visualization layer**
- **Fully Dockerized setup**

The system is designed to be **scalable, extensible, and interview-ready**.

---

## ğŸ§  Architecture

```text
Config (YAML)
   â†“
Airflow DAG
   â†“
Crawling â†’ Extraction â†’ Transformation
   â†“
Aggregation (metrics)
   â†“
Data Contract (summary.json)
   â†“
Streamlit Analytics Dashboard
```

## Project Structure
```bash
Data_Eng_Task/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ website_crawler_dag.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€__init__.py
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ websites.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ metrics/
â”‚       â””â”€â”€ summary.json
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ logs/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore 
â””â”€â”€ README.md
```
