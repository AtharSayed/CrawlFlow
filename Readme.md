# ğŸŒ Scalable Website Crawling & Analytics Pipeline

An **end-to-end, scalable data engineering pipeline** that crawls multiple websites, extracts structured content, aggregates analytics, and visualizes insights through a professional Streamlit dashboard â€” all orchestrated using **Apache Airflow** and **Docker**.

---

##  Project Overview

This project demonstrates how to design and implement a **production-style data pipeline** with clear separation of concerns:

- **Config-driven ingestion** (scale from 1 â†’ N websites)
- **Modular crawling & extraction logic**
- **Airflow-based orchestration**
- **Aggregation layer for analytics**
- **Decoupled visualization layer**
- **Fully Dockerized setup**

The system is designed to be **scalable, extensible, and interview-ready**.

---

## ğŸ§  Architecture

```text
Config (YAML)
   â†“
Airflow DAG
   â†“
Crawling â†’ Extraction â†’ Transformation
   â†“
Aggregation (metrics)
   â†“
Data Contract (summary.json)
   â†“
Streamlit Analytics Dashboard
